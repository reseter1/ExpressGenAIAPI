const { GoogleGenerativeAI } = require('@google/generative-ai');
const { GoogleAIFileManager } = require("@google/generative-ai/server");
const db = require('../models/db.models');
const path = require('path');
const { NULL_AI_PROMPT } = require('../constants/main.constants');
const responseCacheService = require('./response-cache.services');
const moment = require('moment-timezone');

const tryGenerateWithKey = async (apiKey, modelAI, prompt, contextID, req) => {
    const genAI = new GoogleGenerativeAI(apiKey);
    const fileManager = new GoogleAIFileManager(apiKey);
    const model = genAI.getGenerativeModel({ model: modelAI });
    const transaction = await db.sequelize.transaction();
    try {
        if (req.timedout) {
            await transaction.rollback();
            throw new Error('Request timeout');
        }

        const contextRecords = await db.ChatMessages.findAll({
            where: { contextId: contextID },
            order: [['index', 'ASC']],
            transaction
        });

        const fileRecords = await db.Files.findAll({
            where: { contextId: contextID },
            transaction
        });

        const fileHistorys = await Promise.all(fileRecords.map(async (fileRecord) => {
            const fileName = path.basename(fileRecord.filePath);
            const response = await fileManager.uploadFile(path.join(__dirname, '../uploads/', fileRecord.filePath), {
                mimeType: fileRecord.fileMimeType,
                displayName: fileName
            });

            return {
                role: "user",
                parts: [{ text: `File ${fileName}` }, {
                    fileData: {
                        mimeType: response.file.mimeType,
                        fileUri: response.file.uri,
                    },
                }],
            };
        }));

        const context = contextRecords.length || fileHistorys.length ? {
            history: [
                { role: 'user', parts: [{ text: NULL_AI_PROMPT }] },
                { role: 'model', parts: [{ text: 'I will absolutely obey.' }] },
                ...contextRecords.map(record => ({
                    role: record.role,
                    parts: [{ text: record.content }]
                })),
                ...fileHistorys
            ]
        } : null;

        const result = context
            ? await model.startChat(context).sendMessage(prompt)
            : await model.startChat({ history: [{ role: 'user', parts: [{ text: NULL_AI_PROMPT }] }, { role: 'model', parts: [{ text: 'I will absolutely obey.' }] }] }).sendMessage(prompt);

        if (req.timedout) {
            await transaction.rollback();
            throw new Error('Request timeout during processing');
        }

        let createAt = moment.tz("Asia/Ho_Chi_Minh").format("YYYY-MM-DD HH:mm:ss");
        await db.ChatMessages.bulkCreate([
            { contextId: contextID, content: prompt, role: 'user', index: contextRecords.length + 1, createdAt: createAt, updatedAt: createAt },
            { contextId: contextID, content: result.response.text(), role: 'model', index: contextRecords.length + 2, createdAt: createAt, updatedAt: createAt }
        ], { transaction });

        await transaction.commit();
        return { text: result.response.text(), timestamp: createAt };
    } catch (error) {
        console.log(error);
        await transaction.rollback();
        throw error;
    }
};

const tryWithMultipleKeys = async (prompt, modelAI, contextID, req) => {
    const apiKeys = process.env.API_KEY_LIST.split('|');

    for (let apiKey of apiKeys) {
        try {
            if (req.timedout) {
                throw new Error('Request timeout');
            }
            console.log('Current API Key: ', apiKey);
            return await tryGenerateWithKey(apiKey, modelAI, prompt, contextID, req);
        } catch (error) {
            console.log('API Key Error: ', apiKey);
            if (error.message.includes('timeout')) {
                throw error;
            }
        }
    }

    throw new Error('Please try again, the error may be due to an unsupported file type, or the API server is busy');
};

const genService = {
    getAIGenerateWithContext: async (prompt, contextID, modelAI, req) => {
        const cacheKey = `${contextID}_${prompt}_${modelAI}`;
        const cachedResponse = await responseCacheService.get(cacheKey);
        
        if (cachedResponse) {
            return cachedResponse;
        }
        
        const response = await tryWithMultipleKeys(prompt, modelAI, contextID, req);
        
        await responseCacheService.set(cacheKey, response);
        
        return response;
    }
};

module.exports = genService;